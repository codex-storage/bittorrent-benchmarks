apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: deluge-benchmark-
spec:
  serviceAccountName: codex-benchmarks-workflows
  entrypoint: benchmark-workflow
  arguments:
    parameters:
      - name: repetitions
        value: 5
      - name: seederSets
        value: 2
      - name: fileSize
        value: '["100MB", "1GB", "5GB"]'
      - name: constrained__networkSize_seeders
        value: "[[2, 1], [8, [1, 2, 4]], [16, [1, 2, 4, 8]], [32, [1, 2, 4, 8, 16]]]"
      - name: maxExperimentDuration
        value: 144h

  templates:
    - name: benchmark-workflow
      parallelism: 1
      steps:
        - - name: generate-group-id
            template: generate-group-id

        - - name: expand-parameter-matrix
            template: expand-parameter-matrix

        - - name: benchmark-experiment
            template: wrapped-benchmark-experiment
            arguments:
              parameters:
                - name: groupId
                  value: "{{steps.generate-group-id.outputs.result}}"
                - name: runId
                  value: "{{item.runId}}"
                - name: fileSize
                  value: "{{item.fileSize}}"
                - name: seederSets
                  value: "{{item.seederSets}}"
                - name: networkSize
                  value: "{{item.networkSize}}"
                - name: seeders
                  value: "{{item.seeders}}"
                - name: repetitions
                  value: "{{item.repetitions}}"

            withParam: "{{steps.expand-parameter-matrix.outputs.result}}"

    - name: expand-parameter-matrix
      script:
        image: codexstorage/bittorrent-benchmarks-workflows:latest
        command: [ "python", "-m", "parameter_expander" ]
        args:
          - "{{ workflow.parameters.json }}"

    - name: generate-group-id
      script:
        image: codexstorage/bittorrent-benchmarks-workflows:latest
        command: [ "/bin/bash" ]
        source: |
          echo "$(date +%s)"

    # We "wrap" the benchmark workflow with a dummy workflow so exit handlers behave properly. If we
    # were to call benchmark-experiment directly from the main flow, the exit handlers would be run
    # only when the entire set of experiments is done, not when each individual experiment is done.
    - name: wrapped-benchmark-experiment
      inputs:
        parameters:
          - name: groupId
          - name: runId
          - name: fileSize
          - name: seederSets
          - name: networkSize
          - name: seeders
          - name: repetitions
      steps:
        - - name: benchmark-experiment
            template: benchmark-experiment
            hooks:
              exit:
                template: cleanup
                arguments:
                  parameters:
                    - name: runId
                      value: "{{inputs.parameters.runId}}"
            arguments:
              parameters:
                - name: groupId
                  value: "{{inputs.parameters.groupId}}"
                - name: runId
                  value: "{{inputs.parameters.runId}}"
                - name: fileSize
                  value: "{{inputs.parameters.fileSize}}"
                - name: seederSets
                  value: "{{inputs.parameters.seederSets}}"
                - name: networkSize
                  value: "{{inputs.parameters.networkSize}}"
                - name: seeders
                  value: "{{inputs.parameters.seeders}}"
                - name: repetitions
                  value: "{{inputs.parameters.repetitions}}"

    - name: benchmark-experiment
      inputs:
        parameters:
          - name: groupId
          - name: runId
          - name: fileSize
          - name: seederSets
          - name: networkSize
          - name: seeders
          - name: repetitions

      steps:
        - - name: deploy-experiment
            template: deploy-experiment
            arguments:
              parameters:
                - name: groupId
                  value: "{{inputs.parameters.groupId}}"
                - name: runId
                  value: "{{inputs.parameters.runId}}"
                - name: fileSize
                  value: "{{inputs.parameters.fileSize}}"
                - name: seederSets
                  value: "{{inputs.parameters.seederSets}}"
                - name: networkSize
                  value: "{{inputs.parameters.networkSize}}"
                - name: seeders
                  value: "{{inputs.parameters.seeders}}"
                - name: repetitions
                  value: "{{inputs.parameters.repetitions}}"

        - - name: wait-for-experiment
            template: wait-for-experiment
            arguments:
              parameters:
                - name: groupId
                  value: "{{inputs.parameters.groupId}}"
                - name: runId
                  value: "{{inputs.parameters.runId}}"

    - name: deploy-experiment
      inputs:
        parameters:
          - name: groupId
          - name: runId
          - name: fileSize
          - name: seederSets
          - name: networkSize
          - name: seeders
          - name: repetitions

      script:
        image: codexstorage/bittorrent-benchmarks-workflows:latest
        command: [ "/bin/bash" ]
        source: |
          helm install e{{inputs.parameters.runId}} ./k8s/charts/deluge\
            --namespace codex-benchmarks\
            --set experiment.groupId=g{{inputs.parameters.groupId}}\
            --set experiment.repetitions={{inputs.parameters.repetitions}}\
            --set experiment.fileSize={{inputs.parameters.fileSize}}\
            --set experiment.networkSize={{inputs.parameters.networkSize}}\
            --set experiment.seeders={{inputs.parameters.seeders}}\
            --set experiment.seederSets={{inputs.parameters.seederSets}}

    - name: wait-for-experiment
      inputs:
        parameters:
          - name: groupId
          - name: runId
      script:
        image: codexstorage/bittorrent-benchmarks-workflows:latest
        command: [ "/bin/bash" ]
        source: |
          ./docker/bin/kubectl-wait-job\
            --selector=app.kubernetes.io/name=deluge-experiment-runner,\
          app.kubernetes.io/instance=e{{inputs.parameters.runId}},\
          app.kubernetes.io/part-of=g{{inputs.parameters.groupId}}\
            --timeout={{workflow.parameters.maxExperimentDuration}}\
            -n codex-benchmarks

    - name: cleanup
      inputs:
        parameters:
          - name: runId
      script:
        image: codexstorage/bittorrent-benchmarks-workflows:latest
        command: [ "/bin/bash" ]
        source: |
          helm uninstall e{{inputs.parameters.runId}} -n codex-benchmarks
      
